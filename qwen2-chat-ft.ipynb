{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,)\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    data_list = []\n",
    "    with open(filename, \"r\", encoding=\"gb18030\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            i += 1\n",
    "            if i < 10:\n",
    "                print(line)\n",
    "            try:\n",
    "                dept, title, ques, ans = line.strip(\"\\n\").split(',', 4)\n",
    "                data_list.append(\n",
    "                    {\n",
    "                    'department': dept,\n",
    "                    'input': ques,\n",
    "                    'output': ans\n",
    "                    }\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department,title,ask,answer\n",
      "\n",
      "营养保健科,小儿肥胖超重该如何治疗,女宝宝，刚7岁，这一年，察觉到，我家孩子身上肉很多，而且，食量非常的大，平时都不喜欢吃去玩，请问：小儿肥胖超重该如何治疗。,孩子出现肥胖症的情况。家长要通过孩子运功和健康的饮食来缓解他的症状，可以先让他做一些有氧运动，比如慢跑，爬坡，游泳等，并且饮食上孩子多吃黄瓜，胡萝卜，菠菜等，禁止孩子吃一些油炸食品和干果类食物，这些都是干热量高脂肪的食物，而且不要让孩子总是吃完就躺在床上不动，家长在治疗小儿肥胖期间如果孩子情况严重就要及时去医院在医生的指导下给孩子治疗。\n",
      "\n",
      "营养保健科,小儿肥胖超重该怎样医治,男孩子，刚4岁，最近，发现，我家孩子体重要比别的孩子重很多，而且，最近越来越能吃了，还特别的懒，请问：小儿肥胖超重该怎样医治。,孩子一旦患上肥胖症家长要先通过运动和饮食来改变孩子的情况，要让孩子做一些他这个年龄段能做的运动，如游泳，慢跑等，要给孩子多吃一些像苹果，猕猴桃，胡萝卜等食物，禁止孩子吃高热量，高脂肪的食物，像蛋糕，干果，曲奇饼干等，严格的控制孩子的饮食，不要让他暴饮暴食，多运动对改变孩子肥胖都是有好处的，在治疗小儿肥胖期间如果情况严重，建议家长先带孩子去医院检查一下孩子肥胖症的原因在针对性的治疗。\n",
      "\n",
      "营养保健科,小儿肥胖能吃该如何治疗,男宝，已经5岁，今年，察觉到，孩子身上越来越肉乎了，同时，吃的饭也比一般孩子多，平时都不喜欢吃去玩，请问：小儿肥胖能吃该如何治疗。,当孩子患上肥胖症的时候家长可以增加孩子的运动量和控制他的饮食来改变症状，像游泳，爬坡这类游泳运动对肥胖的症状都很好的效果，像冬瓜，西红柿这样高纤维的蔬菜要多吃一些，孩子不可以吃像蛋糕，夏威夷果这些高热量的食物，而且不要让孩子总是吃完就躺在床上不动，家长在治疗小儿肥胖期间如果孩子情况严重就要及时去医院在医生的指导下给孩子治疗。\n",
      "\n",
      "营养保健科,小儿肥胖能吃该如何医治,女宝宝，目前2岁，近期，观察到，我家孩子越来越胖了，而且，吃起来好像也特别不节制，叫他运动也不愿意，请问：小儿肥胖能吃该如何医治。,当孩子患上肥胖症的时候家长可以增加孩子的运动量和控制他的饮食来改变症状，家长要监督孩子做一些有氧运动像慢跑，游泳等，要给孩子多吃一些像苹果，猕猴桃，胡萝卜等食物，一定要禁止孩子吃蛋糕，板栗这些高热量的食物，生活中不要让孩子在床上吃零食或者吃完就躺着这些不好的习惯也会让脂肪堆积，肥胖症治疗期间家长要根据孩子的情况进行合理的治疗，如果病情严重的话一定要去医院查明原因针对治疗。\n",
      "\n",
      "营养保健科,小儿肥胖懒应怎样治疗,男孩，7岁，上小学了，这一年，观察到，孩子身上越来越肉乎了，而且，食量非常的大，平时都不喜欢吃去玩，请问：小儿肥胖懒应怎样治疗。,当孩子患上肥胖症的时候家长可以增加孩子的运动量和控制他的饮食来改变症状，给孩子在承受范围内安排孩子游泳，慢跑等运动，并且多吃一些蔬菜，像西红柿，黄瓜等，不能给孩子吃像夏威夷果，曲奇饼干等高热量的零食，要帮助孩子改正吃完不喜欢动，或者暴饮暴食的习惯，在治疗小儿肥胖期间如果情况严重，建议家长先带孩子去医院检查一下孩子肥胖症的原因在针对性的治疗。\n",
      "\n",
      "营养保健科,小儿肥胖能吃理应如何治效果才好,女宝宝，刚8岁，今年，察觉到，我家孩子体重要比别的孩子重很多，同时，最近越来越能吃了，叫他运动也不愿意，请问：小儿肥胖能吃理应如何治效果才好。,孩子出现肥胖症的情况。家长要通过孩子运功和健康的饮食来缓解他的症状，可以给孩子安排游泳或者慢跑这样的有氧运动，在这期间让孩子吃一些有利于改善肥胖症的蔬菜，例如冬瓜，豌豆黄等，不要让孩子吃一些高热量高脂肪的食物，比如，汉堡薯条，曲奇饼干等，也不要让孩子吃完就睡或者躺在沙发上吃零食，家长在治疗小儿肥胖期间如果孩子情况严重就要及时去医院在医生的指导下给孩子治疗。\n",
      "\n",
      "营养保健科,小儿肥胖能吃该如何治疗好,女宝宝，刚5岁，这一年，觉得，我家孩子体重要比别的孩子重很多，而且，吃起来好像也特别不节制，还特别的懒，请问：小儿肥胖能吃该如何治疗好。,小儿肥胖你可以先给孩子制定运动量和改变饮食，可以先让他做一些有氧运动，比如慢跑，爬坡，游泳等，并且像胡萝卜，菠菜对改变肥胖都是有帮助的，不要给孩子吃炸鸡，蛋糕这类高脂肪的食物，而且不要让孩子总是吃完就躺在床上不动，在给孩子治疗肥胖的期间，如果孩子情况严重一定要带孩子去正规的医院治疗。\n",
      "\n",
      "营养保健科,小儿肥胖懒怎样诊治,我家的孩子是男宝宝，7岁，这一年，觉得，孩子身上越来越肉乎了，另外，一顿可以吃好几碗饭，喜欢吃完饭就躺着，请问：小儿肥胖懒怎样诊治。,如果孩子患有的肥胖症，家长要从他的运动和饮食上面开始，家长可以让孩子做一些有利于改变肥胖症的运动，比如说去慢跑，去游泳等，并且像胡萝卜，菠菜对改变肥胖都是有帮助的，不要让孩子吃一些高热量高脂肪的食物，比如，汉堡薯条，曲奇饼干等，而且孩子不喜欢运动的习惯一定要改掉，不然容易堆积脂肪，在给孩子治疗肥胖的期间，如果孩子情况严重一定要带孩子去正规的医院治疗。\n",
      "\n",
      "84344\n"
     ]
    }
   ],
   "source": [
    "data_list = load_dataset(\"./Chinese-medical-dialogue-data-master/Data_数据/Pediatric_儿科/儿科5-14000.csv\")\n",
    "\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_message(data_list):\n",
    "    '''\n",
    "    格式样例：\n",
    "    [\n",
    "        {\n",
    "            \"id\": \"identity_0\",\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": \"你好\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": \"我是⼀个语⾔模型，我叫通义千问。\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    '''\n",
    "    new_list = []\n",
    "    for i, data in enumerate(data_list):\n",
    "        _id = f\"identity_{i}\"\n",
    "        new_list.append(\n",
    "            {\n",
    "            \"id\": _id,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": data[\"input\"]\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": data[\"output\"]\n",
    "                }\n",
    "        ]\n",
    "    })\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_name(s):\n",
    "    s = s.replace('<NAME>', '智能医⽣客服机器⼈⼩D')\n",
    "    s = s.replace('<AUTHOR>', 'Greedy AI')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "def load_self_cong_data(filename):\n",
    "    data_list = []\n",
    "    for d in json.load(open(filename, \"r\", encoding=\"utf-8\")):\n",
    "        d[\"instruction\"] = replace_name(d[\"instruction\"])\n",
    "        d[\"output\"] = replace_name(d[\"output\"])\n",
    "        data_list.append({\n",
    "            \"id\": random.randint(10000, 100000),\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": d[\"instruction\"]\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": d[\"output\"]\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 46863, 'conversations': [{'from': 'user', 'value': '你好'}, {'from': 'assistant', 'value': '您好，我是 智能医⽣客服机器⼈⼩D，一个由 Greedy AI 打造的人工智能助手，请问有什么可以帮助您的吗？'}]}\n"
     ]
    }
   ],
   "source": [
    "self_cong_data = load_self_cong_data(\"self_cogniton.json\")\n",
    "print(self_cong_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 69944, 'conversations': [{'from': 'user', 'value': '你好'}, {'from': 'assistant', 'value': '您好，我是 智能医⽣客服机器⼈⼩D，一个由 Greedy AI 开发的 AI 助手，很高兴认识您。请问我能为您做些什么？'}]}\n",
      "{'id': 'identity_62691', 'conversations': [{'from': 'user', 'value': '血液细胞检测报告各项指数都正常只是在血液细胞检测报告右上角贴了个小纸片，内容是:医生说最后这项26的指数正常的应该是5以内，说我的女儿有炎症，体温37.7度，浑身发烫脸通红，不哭啼，无厌食症状 曾经的治疗情况和效果：医生说要打点滴个星期，今天第一天打了两小时点滴，好象是消炎的就一瓶，除了一盒小儿解感颗粒和一盒金瓜子颗粒在乎怎样的帮助：您好我女儿的病情真向医生说的那么严重吗，现在除了会一点儿发烫，没别的还是笑眯眯的。谢谢'}, {'from': 'assistant', 'value': '根据报告不是很严重，就是有点上呼吸道感染，打几天点滴就好了，祝您的孩子早日康复。。根据报告不是很严重，就是有点上呼吸道感染，打几天点滴就好了，祝您的孩子早日康复。以上是对“十个月女婴病症解方程！”这个问题的建议，期望对您有帮助，祝您健康！！'}]}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "format_data_list = prepare_message(data_list)\n",
    "random.shuffle(format_data_list)\n",
    "format_data_list = self_cong_data + format_data_list\n",
    "print(format_data_list[0])\n",
    "print(format_data_list[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 84000\n",
      "test data size: 424\n"
     ]
    }
   ],
   "source": [
    "train_data = format_data_list[:84000]\n",
    "test_data = format_data_list[84000:]\n",
    "print(\"train data size:\", len(train_data))\n",
    "print(\"test data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！很高兴能够帮助你。请问有什么可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "index = 10\n",
    "\n",
    "prompt = \"你好\"\n",
    "prompt1 = \"孩⼦积⻝了怎么办？\"\n",
    "prompt2 = \"孩⼦身上⻓疹⼦了是啥原因呢\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to('cuda:0')\n",
    "generated_ids = original_model.generate(model_inputs, max_new_tokens=512)\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs, generated_ids)]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "\n",
    "# response, history = original_model.chat(tokenizer, \"你好\", history=None)\n",
    "# print(response)\n",
    "# response, history = original_model.chat(tokenizer, \"孩⼦积⻝了怎么办？\", history=history)\n",
    "# print(response)\n",
    "# response, history = original_model.chat(tokenizer, \"孩⼦身上⻓疹⼦了是啥原因呢\", history=history)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "def preprocess(\n",
    "        sources,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_len: int,\n",
    "        system_message: str = \"You are a helpful assistant.\"\n",
    "    ):\n",
    "    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "\n",
    "    im_start = 151644 #tokenizer.bos_token_id\n",
    "    print(im_start)\n",
    "    im_end = tokenizer.eos_token_id\n",
    "    print(im_end)\n",
    "    nl_tokens = tokenizer('\\n').input_ids\n",
    "    _system = tokenizer('system').input_ids + nl_tokens\n",
    "    _user = tokenizer('user').input_ids + nl_tokens\n",
    "    _assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "    \n",
    "    # Apply prompt templates\n",
    "    input_ids, targets = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != roles[\"user\"]:\n",
    "            source = source[1:]\n",
    "\n",
    "        input_id, target = [], []\n",
    "        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n",
    "        input_id += system\n",
    "        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n",
    "        assert len(input_id) == len(target)\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens + tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n",
    "            input_id += _input_id\n",
    "            if role == '<|im_start|>user':\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n",
    "            elif role == '<|im_start|>assistant':\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n",
    "                          _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            target += _target\n",
    "        assert len(input_id) == len(target)\n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "        input_ids.append(input_id[:max_len])\n",
    "        targets.append(target[:max_len])\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    targets = torch.tensor(targets, dtype=torch.int)\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "    def __init__(self, raw_data, tokenizer, max_len: int):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer, max_len)\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "        print(\"Formatting done...\")\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, i):\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting inputs...\n",
      "151644\n",
      "151645\n",
      "Formatting done...\n",
      "Formatting inputs...\n",
      "151644\n",
      "151645\n",
      "Formatting done...\n",
      "{'id': 69944, 'conversations': [{'from': 'user', 'value': '你好'}, {'from': 'assistant', 'value': '您好，我是 智能医⽣客服机器⼈⼩D，一个由 Greedy AI 开发的 AI 助手，很高兴认识您。请问我能为您做些什么？'}]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SupervisedDataset(train_data[:1000], tokenizer, max_len=1024)\n",
    "test_dataset = SupervisedDataset(test_data, tokenizer, max_len=1024)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=16,\n",
    "    #target_modules=[\"c_attn\", \"c_proj\", \"w1\", \"w2\"],\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05, # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "#original_model.gradient_checkpointing_enable()\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "peft_model = get_peft_model(original_model, config)\n",
    "original_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir = './checkpoints_self_cong-qwen2-1_5b/'\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    max_steps=1000,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1001,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "peft_model.config.use_cache = False\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 10%|█         | 100/1000 [00:51<07:45,  1.93it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1139, 'grad_norm': 1.0551625490188599, 'learning_rate': 0.00018018018018018018, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 20%|██        | 200/1000 [01:44<06:51,  1.94it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7448, 'grad_norm': 1.0395786762237549, 'learning_rate': 0.00016016016016016018, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 30%|███       | 300/1000 [02:37<06:00,  1.94it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8901, 'grad_norm': 0.934403121471405, 'learning_rate': 0.00014014014014014013, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 40%|████      | 400/1000 [03:30<05:10,  1.93it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9392, 'grad_norm': 0.6916159391403198, 'learning_rate': 0.00012012012012012013, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 50%|█████     | 500/1000 [04:23<04:20,  1.92it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9117, 'grad_norm': 1.0845364332199097, 'learning_rate': 0.00010010010010010012, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 60%|██████    | 600/1000 [05:16<03:28,  1.91it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7625, 'grad_norm': 0.6826773285865784, 'learning_rate': 8.008008008008009e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 70%|███████   | 700/1000 [06:09<02:36,  1.92it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7058, 'grad_norm': 0.8416334390640259, 'learning_rate': 6.0060060060060066e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 80%|████████  | 800/1000 [07:02<01:44,  1.92it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7114, 'grad_norm': 0.9977681636810303, 'learning_rate': 4.0040040040040046e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 90%|█████████ | 900/1000 [07:55<00:52,  1.92it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7907, 'grad_norm': 0.6769275069236755, 'learning_rate': 2.0020020020020023e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [08:48<00:00,  1.93it/s]/opt/Envs/miniconda3/envs/torch2.2/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7318, 'grad_norm': 0.79654461145401, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:49<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 529.9876, 'train_samples_per_second': 1.887, 'train_steps_per_second': 1.887, 'train_loss': 1.830198486328125, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.830198486328125, metrics={'train_runtime': 529.9876, 'train_samples_per_second': 1.887, 'train_steps_per_second': 1.887, 'train_loss': 1.830198486328125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "统计参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainable model parameters: 36929536\\n             all model parameters: 925545984\\n             percentage of trainable model parameters:             3.99%'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\n \\\n",
    "            all model parameters: {all_model_params}\\n \\\n",
    "            percentage of trainable model parameters: \\\n",
    "            {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print_number_of_trainable_model_parameters(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以重启一下内核，释放显存。否则显存小的话容易OOM。\n",
    "重启的话再执行一下最开始的导入包的cell。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_path = \"/mnt/AlgoTempData1/llm-weights/qwen/Qwen2-1_5B-Instruct\"\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "output_dir = './checkpoints_self_cong-qwen2-1_5b/'\n",
    "ft_model = PeftModel.from_pretrained(\n",
    "    original_model,\n",
    "    output_dir + '/checkpoint-1000',\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map={\"\": 0},\n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "婴幼儿时期皮肤的角质层较薄，对刺激的敏感性也较高。而宝宝又比较活泼好动，因此，一旦接触到某些不洁的东西，就容易引发湿疹等过敏性疾病。另外，一些过敏原、刺激物也会引起孩子患湿疹。比如：衣物洗涤剂中的色素；奶制品中添加的人工香精或防腐剂；护肤品中有害成分的重金属如铅、汞等等，都会诱发婴儿湿疹。指导意见：\n",
      "      1.首先要观察有无其它不适的症状，如有发烫、发热则需排除感染的可能性；\n",
      "      2.对于过敏体质者，平时应尽量避免接触致敏物质，外出时最好带上口罩，防止吸入花粉、灰尘等物质；\n",
      "      3.勤剪指甲，勤洗手，并用温水浸泡手脚；\n",
      "      4.如果孩子患的是特殊类型的皮炎，在医生指导下服用抗过敏药。 \n",
      "      以上是对“新生儿身上出现红点是什么原因引起的”这个问题的建议，希望对您有帮助，祝您健康！\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"孩⼦身上⻓疹⼦了是啥原因呢\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to('cuda:0')\n",
    "generated_ids = ft_model.generate(model_inputs, max_new_tokens=512)\n",
    "\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs, generated_ids)]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
